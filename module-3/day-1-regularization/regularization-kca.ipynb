{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge & Lasso & Regularization\n",
    "\n",
    "## Objectives \n",
    "<a name=\"objectives\"></a>\n",
    "\n",
    "- List methods other than r-squared to assess model fit\n",
    "\n",
    "- Describe regularization's role in regression\n",
    "\n",
    "- Summarize the difference between L1 and L2 norms\n",
    "\n",
    "- Understand the effect of hyper-parameter $\\alpha$ in Ridge and Lasso.\n",
    "\n",
    "- Compare and contrast between Lasso-Ridge-Linear models.\n",
    "\n",
    "- Apply Lasso and Ridge with sklearn and understand the parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Review \n",
    "## Review Linear Regression once again:\n",
    "![imune](https://media3.giphy.com/media/l4FGBBs6fjtZXzXCE/giphy.gif?cid=ecf05e47dbb74b8bcd3cf3aafbc86e24452503799b466084&rid=giphy.gif)\n",
    "\n",
    "\n",
    "__Linear Model__\n",
    "\n",
    "\n",
    "$$ Y = w_{0} + w_{1}X_1 + w_{2}X_{2} + \\cdots + w_{p}X_{p} + \\varepsilon $$\n",
    "\n",
    " - We train model to understand the paramaters $w_{i}$ \n",
    " \n",
    " - Use linear algebra or gradient descent to find parameters to minimize:\n",
    " \n",
    " Note that the predictions are given by:\n",
    " \n",
    " $$ \\hat{y}_{i} =  w_{0} + w_{1}X_{i1} + w_{2}X_{i2} + \\cdots + w_{p}X_{i_p}$$\n",
    " \n",
    " Therefore individual errors are given by:\n",
    " \n",
    " $$ e_{i} = y_{i} - \\hat{y}_{i} $$\n",
    " \n",
    " As a result, the residual sum of squares can be expressed as:\n",
    " \n",
    " $$ RSS(\\boldsymbol{w}) = \\sum\\limits_{i=0}^{N} e_{i}^{2}$$\n",
    " \n",
    " \n",
    " \n",
    " $$ J(\\boldsymbol{w}) = \\sum\\limits_{i=0}^{N} (y_{i} - w_{0} - w_{1}X_{i1} - w_{2}X_{i2} - \\cdots - w_{p}X_{i_p})^{2} $$\n",
    " \n",
    " And this equation can be written in short hand as:\n",
    " \n",
    " $$ J(\\boldsymbol{w}) = \\rvert \\boldsymbol{y} - X \\boldsymbol{w} \\rvert^{2} $$\n",
    " \n",
    " or with betas as we are used to seeing them:\n",
    " \n",
    " $$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(b_jx_{ij} + b))^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What methods or metrics have you learned to assess model fit?\n",
    "\n",
    "\n",
    "## Another Question: How do those metrics feel about adding more variables to the model?\n",
    "\n",
    "![schitts](https://media1.giphy.com/media/fXtGlVSI2ZB2E1JO0b/giphy.gif?cid=ecf05e47f55093929090866ffd59873c647521e25e164830&rid=giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "<a name=\"review\"></a>\n",
    "\n",
    "\n",
    "\n",
    "[__Overfitting - Underfitting__](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "<img src=\"underfitting_overfitting.png\" alt=\"Bias-Variance\" style=\"width: 500px;\"/>\n",
    "\n",
    "[__Bias - Variance Trade-Off__](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "<img src=\"bias_variance_trade_off.png\" alt=\"Bias-Variance\" style=\"width: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Tools:\n",
    "- AIC & BIC to compare models\n",
    "- Regularization to produce a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: AIC & BIC\n",
    "## Calculating AIC and BIC \n",
    "AIC and BIC are information criteria for evaluating model performance. These measures compute the goodness of fit with the estimated parameters, but apply a penalty function on the number of parameters in the model.\n",
    "\n",
    "\n",
    "The BIC is also known as the _Schwarz information criterion (abrv. SIC)_ or the Schwarz-Bayesian information criteria. The AIC is also known as the Akaike information criterion. Both criterion were developed in the 1970s.\n",
    "\n",
    "- AIC is defined as: $2k - 2log(L)$\n",
    "- BIC is defined as: $klog(n) - 2log(L)$  \n",
    "\n",
    "$n$ = sample size <br>\n",
    "$k$ = variables in model <br>\n",
    "$L$ = log of sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def aic(y, y_pred, k):\n",
    "    resid = y - y_pred\n",
    "    sse = (resid**2).sum()\n",
    "    AIC = 2*k - 2*np.log(sse)\n",
    "    \n",
    "    return AIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bic(y, y_pred, k, n):\n",
    "    resid = y - y_pred\n",
    "    sse = (resid**2).sum()\n",
    "    BIC = np.log(n) + n*np.log(sse/n)\n",
    "    \n",
    "    return BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIC & BIC in comparsion\n",
    "\n",
    "Both AIC and BIC are only useful in comparing the performance of two different model specifications and **cannot** be used on their own. \n",
    "\n",
    "_**Lower**_ AIC and BIC are better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularization\n",
    "\n",
    "## Regularization Techniques\n",
    "\n",
    "\n",
    "- Why?\n",
    "\n",
    "    - Reduces complexity\n",
    "    \n",
    "    - Reduce the chance of overfitting.\n",
    "    \n",
    "    - Reduces model's variance at the expense of introducing small bias\n",
    "    \n",
    "    - Increases model's interprettability.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Ridge regularization (L2 Norm)\n",
    "![ridge](https://media2.giphy.com/media/3Aie9MmJ0klyM/giphy.gif?cid=ecf05e472724cf8b955fe4da2a1050dd8865bec22b629736&rid=giphy.gif)\n",
    "\n",
    "Instead of minimizing $J(w)$ (least squares method), we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p} w_{i}^{2} $$\n",
    "\n",
    "Function is in sklearn as [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "The **goal** of ridge regression is to find  the right alpha that best manages multicolinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge regression applies a penalizing parameter $\\lambda$ *slope* $^2$, such that a small bias will be introduced to the entire model depending on the value of $\\lambda$, which is called a *hyperparameter*. \n",
    "\n",
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(b_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p b_j^2$$\n",
    "\n",
    "The result of applying such a penalizing parameter to the cost function, resulting a different regression model that minimizing the residual sum of square **and** the term $\\lambda \\sum_{j=1}^p b_j^2$. \n",
    "\n",
    "The Ridge regression improves the fit of the original regression line by introducing some bias/changing the slope and intercept of the original line. Recall the way we interpret a regression model Y = mx + b: with every unit increase in x, the outcome y increase by m unit. Therefore, the bigger the coefficient m is, the more the outcome is subjected to changes in predictor x. Ridge regression works by reducing the magnitude of the coefficient m and therefore reducing the effect the predictors have on the outcome. Let's look at a simple example.\n",
    "\n",
    "The ridge regression penalty term contains all of the coefficients squared from the original regression line except for the intercept term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2a: Lasso regularization (L1 Norm)\n",
    "![lasso](https://media3.giphy.com/media/wRKeX8o1eIxxu/giphy.gif?cid=ecf05e47a70490d9dee94f19dd8876390c0ef88243356922&rid=giphy.gif)\n",
    "\n",
    "Instead of minimizing $J(\\boldsymbol{\\omega})$, we will minimize:\n",
    "\n",
    "$$ J_{\\alpha}(\\boldsymbol{w}) = J(\\boldsymbol{w}) + \\alpha\\sum_{i=1}^{p}| w_{i} | $$\n",
    "\n",
    "Function in skelarn as [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n",
    "The **goal** of lasso regression is to obtain the subset of predictors and increase model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is very similar to Ridge regression except for one difference - the penalty term is not squared but the absolute values of the coefficients muliplied by lambda, expressed by:\n",
    "\n",
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(b_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p \\mid b_j \\mid$$\n",
    "\n",
    "The biggest difference in Ridge and Lasso is that Lasso simultaneously performs variable selection: some coefficients are shrunk to 0, rendering them nonexistence in the original regression model. Therefore, Lasso regression performs very well when you have higher dimensional dataset where some predictors are useless; whereas Ridge works best when all the predictors are needed. \n",
    "\n",
    "<img src=\"https://media.giphy.com/media/AWeYSE0qgpk76/giphy.gif\" width= \"400\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# implementation \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = sns.load_dataset('mpg')\n",
    "\n",
    "#data = pd.read_csv(\"https://raw.githubusercontent.com/learn-co-curriculum/dsc-2-24-09-ridge-and-lasso-regression/master/auto-mpg.csv\") \n",
    "data = data.sample(50)\n",
    "y = data[[\"mpg\"]]\n",
    "X = data.drop([\"mpg\", \"name\", \"origin\"], axis=1)\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "transformed = scale.fit_transform(X)\n",
    "X = pd.DataFrame(transformed, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 9)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2575</td>\n",
       "      <td>16.2</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>pontiac j2000 se hatchback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>13.0</td>\n",
       "      <td>71</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth fury iii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>21.5</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2720</td>\n",
       "      <td>13.5</td>\n",
       "      <td>77</td>\n",
       "      <td>japan</td>\n",
       "      <td>mazda rx-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>20.6</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3360</td>\n",
       "      <td>16.6</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge aspen 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2875</td>\n",
       "      <td>17.0</td>\n",
       "      <td>74</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford maverick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>37.7</td>\n",
       "      <td>4</td>\n",
       "      <td>89.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2050</td>\n",
       "      <td>17.3</td>\n",
       "      <td>81</td>\n",
       "      <td>japan</td>\n",
       "      <td>toyota tercel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>21.0</td>\n",
       "      <td>6</td>\n",
       "      <td>155.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>2472</td>\n",
       "      <td>14.0</td>\n",
       "      <td>73</td>\n",
       "      <td>usa</td>\n",
       "      <td>mercury capri v6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>4141</td>\n",
       "      <td>14.0</td>\n",
       "      <td>74</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford gran torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>44.3</td>\n",
       "      <td>4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2085</td>\n",
       "      <td>21.7</td>\n",
       "      <td>80</td>\n",
       "      <td>europe</td>\n",
       "      <td>vw rabbit c (diesel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>29.5</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2135</td>\n",
       "      <td>16.6</td>\n",
       "      <td>78</td>\n",
       "      <td>japan</td>\n",
       "      <td>honda accord lx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>340.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3609</td>\n",
       "      <td>8.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth 'cuda 340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>232.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2945</td>\n",
       "      <td>16.0</td>\n",
       "      <td>73</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc hornet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>2108</td>\n",
       "      <td>15.5</td>\n",
       "      <td>74</td>\n",
       "      <td>europe</td>\n",
       "      <td>fiat 128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>28.4</td>\n",
       "      <td>4</td>\n",
       "      <td>151.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2670</td>\n",
       "      <td>16.0</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>buick skylark limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>17.0</td>\n",
       "      <td>6</td>\n",
       "      <td>163.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>3140</td>\n",
       "      <td>13.6</td>\n",
       "      <td>78</td>\n",
       "      <td>europe</td>\n",
       "      <td>volvo 264gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>3139</td>\n",
       "      <td>14.5</td>\n",
       "      <td>71</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford mustang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>4382</td>\n",
       "      <td>13.5</td>\n",
       "      <td>70</td>\n",
       "      <td>usa</td>\n",
       "      <td>dodge d200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>4363</td>\n",
       "      <td>13.0</td>\n",
       "      <td>73</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford ltd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>34.0</td>\n",
       "      <td>4</td>\n",
       "      <td>112.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2395</td>\n",
       "      <td>18.0</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet cavalier 2-door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3730</td>\n",
       "      <td>19.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc matador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>15.3</td>\n",
       "      <td>82</td>\n",
       "      <td>europe</td>\n",
       "      <td>volkswagen rabbit l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2220</td>\n",
       "      <td>16.9</td>\n",
       "      <td>76</td>\n",
       "      <td>europe</td>\n",
       "      <td>opel 1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>305.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3840</td>\n",
       "      <td>15.4</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet caprice classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>20.5</td>\n",
       "      <td>6</td>\n",
       "      <td>231.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>3425</td>\n",
       "      <td>16.9</td>\n",
       "      <td>77</td>\n",
       "      <td>usa</td>\n",
       "      <td>buick skylark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>1965</td>\n",
       "      <td>15.7</td>\n",
       "      <td>82</td>\n",
       "      <td>japan</td>\n",
       "      <td>honda civic (auto)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>3264</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth valiant custom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>40.8</td>\n",
       "      <td>4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2110</td>\n",
       "      <td>19.2</td>\n",
       "      <td>80</td>\n",
       "      <td>japan</td>\n",
       "      <td>datsun 210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>27.4</td>\n",
       "      <td>4</td>\n",
       "      <td>121.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2670</td>\n",
       "      <td>15.0</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc spirit dl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>29.0</td>\n",
       "      <td>4</td>\n",
       "      <td>90.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1937</td>\n",
       "      <td>14.0</td>\n",
       "      <td>75</td>\n",
       "      <td>europe</td>\n",
       "      <td>volkswagen rabbit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>23.5</td>\n",
       "      <td>6</td>\n",
       "      <td>173.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2725</td>\n",
       "      <td>12.6</td>\n",
       "      <td>81</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet citation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>258.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2962</td>\n",
       "      <td>13.5</td>\n",
       "      <td>71</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc hornet sportabout (sw)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6</td>\n",
       "      <td>232.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2901</td>\n",
       "      <td>16.0</td>\n",
       "      <td>74</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc hornet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>18.0</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>3121</td>\n",
       "      <td>16.5</td>\n",
       "      <td>73</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth valiant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8</td>\n",
       "      <td>360.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>3821</td>\n",
       "      <td>11.0</td>\n",
       "      <td>73</td>\n",
       "      <td>usa</td>\n",
       "      <td>amc ambassador brougham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25.0</td>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2228</td>\n",
       "      <td>14.0</td>\n",
       "      <td>71</td>\n",
       "      <td>japan</td>\n",
       "      <td>toyota corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>46.6</td>\n",
       "      <td>4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2110</td>\n",
       "      <td>17.9</td>\n",
       "      <td>80</td>\n",
       "      <td>japan</td>\n",
       "      <td>mazda glc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>20.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2279</td>\n",
       "      <td>19.0</td>\n",
       "      <td>73</td>\n",
       "      <td>japan</td>\n",
       "      <td>toyota carina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>4668</td>\n",
       "      <td>11.5</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>pontiac catalina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>17.6</td>\n",
       "      <td>6</td>\n",
       "      <td>225.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3465</td>\n",
       "      <td>16.6</td>\n",
       "      <td>81</td>\n",
       "      <td>usa</td>\n",
       "      <td>chrysler lebaron salon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>36.4</td>\n",
       "      <td>5</td>\n",
       "      <td>121.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2950</td>\n",
       "      <td>19.9</td>\n",
       "      <td>80</td>\n",
       "      <td>europe</td>\n",
       "      <td>audi 5000s (diesel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford mustang gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6</td>\n",
       "      <td>250.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3432</td>\n",
       "      <td>21.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>mercury monarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>30.0</td>\n",
       "      <td>4</td>\n",
       "      <td>88.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2065</td>\n",
       "      <td>14.5</td>\n",
       "      <td>71</td>\n",
       "      <td>europe</td>\n",
       "      <td>fiat 124b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>13.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>3169</td>\n",
       "      <td>12.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>ford mustang ii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>16.5</td>\n",
       "      <td>8</td>\n",
       "      <td>351.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>3955</td>\n",
       "      <td>13.2</td>\n",
       "      <td>79</td>\n",
       "      <td>usa</td>\n",
       "      <td>mercury grand marquis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>32.4</td>\n",
       "      <td>4</td>\n",
       "      <td>107.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2290</td>\n",
       "      <td>17.0</td>\n",
       "      <td>80</td>\n",
       "      <td>japan</td>\n",
       "      <td>honda accord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>4440</td>\n",
       "      <td>14.0</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>chevrolet bel air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4498</td>\n",
       "      <td>14.5</td>\n",
       "      <td>75</td>\n",
       "      <td>usa</td>\n",
       "      <td>plymouth grand fury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>79.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>74</td>\n",
       "      <td>europe</td>\n",
       "      <td>fiat x1.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "370  31.0          4         112.0        85.0    2575          16.2   \n",
       "41   14.0          8         318.0       150.0    4096          13.0   \n",
       "243  21.5          3          80.0       110.0    2720          13.5   \n",
       "284  20.6          6         225.0       110.0    3360          16.6   \n",
       "126  21.0          6         200.0         NaN    2875          17.0   \n",
       "348  37.7          4          89.0        62.0    2050          17.3   \n",
       "113  21.0          6         155.0       107.0    2472          14.0   \n",
       "136  16.0          8         302.0       140.0    4141          14.0   \n",
       "325  44.3          4          90.0        48.0    2085          21.7   \n",
       "279  29.5          4          98.0        68.0    2135          16.6   \n",
       "11   14.0          8         340.0       160.0    3609           8.0   \n",
       "99   18.0          6         232.0       100.0    2945          16.0   \n",
       "147  24.0          4          90.0        75.0    2108          15.5   \n",
       "3    16.0          8         304.0       150.0    3433          12.0   \n",
       "305  28.4          4         151.0        90.0    2670          16.0   \n",
       "275  17.0          6         163.0       125.0    3140          13.6   \n",
       "48   18.0          6         250.0        88.0    3139          14.5   \n",
       "27   11.0          8         318.0       210.0    4382          13.5   \n",
       "92   13.0          8         351.0       158.0    4363          13.0   \n",
       "369  34.0          4         112.0        88.0    2395          18.0   \n",
       "162  15.0          6         258.0       110.0    3730          19.0   \n",
       "375  36.0          4         105.0        74.0    1980          15.3   \n",
       "183  25.0          4         116.0        81.0    2220          16.9   \n",
       "285  17.0          8         305.0       130.0    3840          15.4   \n",
       "226  20.5          6         231.0       105.0    3425          16.9   \n",
       "384  32.0          4          91.0        67.0    1965          15.7   \n",
       "152  19.0          6         225.0        95.0    3264          16.0   \n",
       "324  40.8          4          85.0        65.0    2110          19.2   \n",
       "296  27.4          4         121.0        80.0    2670          15.0   \n",
       "175  29.0          4          90.0        70.0    1937          14.0   \n",
       "341  23.5          6         173.0       110.0    2725          12.6   \n",
       "45   18.0          6         258.0       110.0    2962          13.5   \n",
       "127  19.0          6         232.0       100.0    2901          16.0   \n",
       "97   18.0          6         225.0       105.0    3121          16.5   \n",
       "96   13.0          8         360.0       175.0    3821          11.0   \n",
       "31   25.0          4         113.0        95.0    2228          14.0   \n",
       "322  46.6          4          86.0        65.0    2110          17.9   \n",
       "108  20.0          4          97.0        88.0    2279          19.0   \n",
       "156  16.0          8         400.0       170.0    4668          11.5   \n",
       "366  17.6          6         225.0        85.0    3465          16.6   \n",
       "327  36.4          5         121.0        67.0    2950          19.9   \n",
       "393  27.0          4         140.0        86.0    2790          15.6   \n",
       "154  15.0          6         250.0        72.0    3432          21.0   \n",
       "52   30.0          4          88.0        76.0    2065          14.5   \n",
       "166  13.0          8         302.0       129.0    3169          12.0   \n",
       "287  16.5          8         351.0       138.0    3955          13.2   \n",
       "337  32.4          4         107.0        72.0    2290          17.0   \n",
       "157  15.0          8         350.0       145.0    4440          14.0   \n",
       "158  16.0          8         318.0       150.0    4498          14.5   \n",
       "151  31.0          4          79.0        67.0    2000          16.0   \n",
       "\n",
       "     model_year  origin                        name  \n",
       "370          82     usa  pontiac j2000 se hatchback  \n",
       "41           71     usa           plymouth fury iii  \n",
       "243          77   japan                  mazda rx-4  \n",
       "284          79     usa               dodge aspen 6  \n",
       "126          74     usa               ford maverick  \n",
       "348          81   japan               toyota tercel  \n",
       "113          73     usa            mercury capri v6  \n",
       "136          74     usa            ford gran torino  \n",
       "325          80  europe        vw rabbit c (diesel)  \n",
       "279          78   japan             honda accord lx  \n",
       "11           70     usa          plymouth 'cuda 340  \n",
       "99           73     usa                  amc hornet  \n",
       "147          74  europe                    fiat 128  \n",
       "3            70     usa               amc rebel sst  \n",
       "305          79     usa       buick skylark limited  \n",
       "275          78  europe                 volvo 264gl  \n",
       "48           71     usa                ford mustang  \n",
       "27           70     usa                  dodge d200  \n",
       "92           73     usa                    ford ltd  \n",
       "369          82     usa   chevrolet cavalier 2-door  \n",
       "162          75     usa                 amc matador  \n",
       "375          82  europe         volkswagen rabbit l  \n",
       "183          76  europe                   opel 1900  \n",
       "285          79     usa   chevrolet caprice classic  \n",
       "226          77     usa               buick skylark  \n",
       "384          82   japan          honda civic (auto)  \n",
       "152          75     usa     plymouth valiant custom  \n",
       "324          80   japan                  datsun 210  \n",
       "296          79     usa               amc spirit dl  \n",
       "175          75  europe           volkswagen rabbit  \n",
       "341          81     usa          chevrolet citation  \n",
       "45           71     usa  amc hornet sportabout (sw)  \n",
       "127          74     usa                  amc hornet  \n",
       "97           73     usa            plymouth valiant  \n",
       "96           73     usa     amc ambassador brougham  \n",
       "31           71   japan               toyota corona  \n",
       "322          80   japan                   mazda glc  \n",
       "108          73   japan               toyota carina  \n",
       "156          75     usa            pontiac catalina  \n",
       "366          81     usa      chrysler lebaron salon  \n",
       "327          80  europe         audi 5000s (diesel)  \n",
       "393          82     usa             ford mustang gl  \n",
       "154          75     usa             mercury monarch  \n",
       "52           71  europe                   fiat 124b  \n",
       "166          75     usa             ford mustang ii  \n",
       "287          79     usa       mercury grand marquis  \n",
       "337          80   japan                honda accord  \n",
       "157          75     usa           chevrolet bel air  \n",
       "158          75     usa         plymouth grand fury  \n",
       "151          74  europe                   fiat x1.9  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform test train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpenalized Linear Regression Coefficients are:[[  3.1821277   -6.82196982  -5.87933769 -14.5730968    0.28495886\n",
      "    6.71587176]]\n",
      "Unpenalized Linear Regression Intercept:[29.27174824]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unpenalized Linear Regression Coefficients are:{}\".format(lin.coef_))\n",
    "print(\"Unpenalized Linear Regression Intercept:{}\".format(lin.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Coefficients are:[-0.         -9.36432448 -0.         -8.01580738  0.          3.37999001]\n",
      "Lasso Linear Regression Intercept:[29.2673401]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso Regression Coefficients are:{}\".format(lasso.coef_))\n",
    "print(\"Lasso Linear Regression Intercept:{}\".format(lasso.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Coefficients are:[[-1.93402308 -6.35002334 -5.22912606 -8.87489227  0.65693145  6.00233573]]\n",
      "Ridge Linear Regression Intercept:[29.51408323]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ridge Regression Coefficients are:{}\".format(ridge.coef_))\n",
    "print(\"Ridge Linear Regression Intercept:{}\".format(ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create predictions\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train),(40,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test),(10,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 1)\n",
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "print(y_h_ridge_train.shape)\n",
    "print(y_h_ridge_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_h_lasso_train))\n",
    "print(type(y_h_ridge_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the Residual for Ridge, Lasso, and Unpenalized Regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Ridge Model mpg    412.158363\n",
      "dtype: float64\n",
      "Test Error Ridge Model mpg    234.517574\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Lasso Model mpg    572.652118\n",
      "dtype: float64\n",
      "Test Error Lasso Model mpg    372.793464\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Unpenalized Linear Model mpg    386.602549\n",
      "dtype: float64\n",
      "Test Error Unpenalized Linear Model mpg    199.461381\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# examine the residual sum of sq\n",
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2c: Crossvalidation to Optimize the Regularization Hyperparameter\n",
    "\n",
    "The regularization strength could sensibly be any nonnegative number, so there's no way to check \"all possible\" values. It's often useful to try several values that are different orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Gentoo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-70daa2f8c1a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# upate the names of your datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \"\"\"\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    545\u001b[0m                          \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_accept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                          \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                          multi_output=True, y_numeric=True)\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sparse_cg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Gentoo'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "alphas = [1, 10, 100, 1000, 10000]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha, random_state=42)\n",
    "    # upate the names of your datasets\n",
    "    rr.fit(X_train, y_train)\n",
    "    train_score = rr.score(X_train, y_train)\n",
    "    test_score = cross_val_score(rr, X_test, y_test).mean()\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-0ad69ca8d98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Regularization strength $\\lambda$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'$R^2$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m         \"\"\"\n\u001b[1;32m   1664\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 270\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "fig, ax = plt.subplots()\n",
    "plt.xscale('log')\n",
    "plt.title('Ridge $R^2$ as a function of regularization strength')\n",
    "ax.set_xlabel('Regularization strength $\\lambda$')\n",
    "ax.set_ylabel('$R^2$')\n",
    "ax.plot(alphas, train_scores, label='train')\n",
    "ax.plot(alphas, test_scores, label='test')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Practice\n",
    "\n",
    "__Your Turn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>culmen_length_mm</th>\n",
       "      <th>culmen_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>x0_Chinstrap</th>\n",
       "      <th>x0_Gentoo</th>\n",
       "      <th>x1_Dream</th>\n",
       "      <th>x1_Torgersen</th>\n",
       "      <th>x2_MALE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>55.9</td>\n",
       "      <td>17.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>43.6</td>\n",
       "      <td>13.9</td>\n",
       "      <td>217.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38.8</td>\n",
       "      <td>20.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>47.5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>53.5</td>\n",
       "      <td>19.9</td>\n",
       "      <td>205.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     culmen_length_mm  culmen_depth_mm  flipper_length_mm  x0_Chinstrap  \\\n",
       "321              55.9             17.0              228.0           0.0   \n",
       "265              43.6             13.9              217.0           0.0   \n",
       "36               38.8             20.0              190.0           0.0   \n",
       "308              47.5             14.0              212.0           0.0   \n",
       "191              53.5             19.9              205.0           1.0   \n",
       "\n",
       "     x0_Gentoo  x1_Dream  x1_Torgersen  x2_MALE  \n",
       "321        1.0       0.0           0.0      1.0  \n",
       "265        1.0       0.0           0.0      0.0  \n",
       "36         0.0       1.0           0.0      1.0  \n",
       "308        1.0       0.0           0.0      0.0  \n",
       "191        0.0       1.0           0.0      1.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "birds = sns.load_dataset('penguins')\n",
    "\n",
    "birds = birds.dropna(axis = 0, how = 'any')\n",
    "# For simplicity's sake we'll limit our analysis to the numeric columns.\n",
    "\n",
    "#numeric = birds[['culmen_length_mm', 'culmen_depth_mm',\n",
    "                # 'flipper_length_mm', 'body_mass_g']]\n",
    "\n",
    "\n",
    "# We'll drop the rows with null values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(birds.drop('body_mass_g',\n",
    "                                                              axis=1),\n",
    "                                                   birds['body_mass_g'],\n",
    "                                                   random_state=42)\n",
    "\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "dummies = ohe.fit_transform(X_train[['species', 'island', 'sex']])\n",
    "dummies_df = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names(),\n",
    "                         index=X_train.index)\n",
    "X_train_df = pd.concat([X_train[['culmen_length_mm', 'culmen_depth_mm',\n",
    "                                'flipper_length_mm']], dummies_df], axis=1)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try at least 3 different model specifications, using polynomials, interaction terms, etc\n",
    "- Group 1: Use AIC and BIC to pick your \"best\" model specification using Linear Regression\n",
    "- Group 2: Use Ridge AND experiement with different values of alpha to find the \"best\" model. Compare MSE in train and test for each model specification.\n",
    "- Group 3: Use Lasso AND experiment with different values of alpha to find the \"best\" model. Compare MSE in train and test for each model specification.\n",
    "- Report your observations.\n",
    "- A graph comparing your findings and suggesting a model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Summary\n",
    "\n",
    "#### Effect of $\\alpha$ in Lasso and Ridge\n",
    "\n",
    "<img src=\"lasso_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"ridge_effect_of_lambda.png\" alt=\"Lasso-Lambda\" style=\"width: 500px;\"/>\n",
    "\n",
    "<a name='questions'></a>\n",
    "### Questions\n",
    "\n",
    "\n",
    "\n",
    "Q. Should I do normalization for Lasso or Ridge?\n",
    "\n",
    "A. Yes? Why?\n",
    "\n",
    "Q. When we know that Ridge and Lasso is better than vanilla linear regression?\n",
    "\n",
    "A. High variation in your model --> Colinearity and too many variables.\n",
    "\n",
    "Q. How do we know whether we should choose Lasso or Ridge?\n",
    "\n",
    "A. Most of the time they perform very similar but Lasso has the feature selection property, ridge doesn't have this.\n",
    "\n",
    "Q: How do we choose $\\lambda$?\n",
    "\n",
    "A. [sklearn gridsearch](https://scikit-learn.org/stable/modules/grid_search.html#grid-search) for small models or random grid search for bigger models.\n",
    "\n",
    "#### Appendix\n",
    "<a name='appendix'></a>\n",
    "\n",
    "Here I would like to add some reading material that I found useful while working with the code.\n",
    "\n",
    "\n",
    "-  [On ridge and lasso](https://bradleyboehmke.github.io/HOML/regularized-regression.html)\n",
    "\n",
    "- [pd.get_dummies or OneHotEncoder? - Read second answer](https://stackoverflow.com/questions/36631163/pandas-get-dummies-vs-sklearns-onehotencoder-what-are-the-pros-and-cons)\n",
    "\n",
    "- [On dummy variable trap](https://www.algosome.com/articles/dummy-variable-trap-regression.html)\n",
    "\n",
    "- [sklearn.preprocessing.PolynomialFeatures documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "- [A great notebook on Lasso and Ridge](https://github.com/gokererdogan/JaverianaMLCourse/blob/master/Lectures/05.pdf)\n",
    "\n",
    "- [Another good blog post on Lasso and Ridge](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)\n",
    "\n",
    "- Learn.co -- Section-28 Lasso-Ridge\n",
    "\n",
    "- [Toward Datascience Article](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)\n",
    "\n",
    "- [ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) 2.2.2 The Bias-Variance Trade-off and 6.2 Shrinkage Methods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Image Sources in order of appearance: \n",
    "- https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html\n",
    "\n",
    "- https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
